{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions & word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, you'll learn natural language processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:21:02.772571Z",
     "start_time": "2021-10-21T07:21:02.757285Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Practicing regular expressions: re.split() and re.findall()**\n",
    "\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:21:03.365135Z",
     "start_time": "2021-10-21T07:21:03.228458Z"
    }
   },
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:21:03.551841Z",
     "start_time": "2021-10-21T07:21:03.376167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:21:04.565641Z",
     "start_time": "2021-10-21T07:21:03.552891Z"
    }
   },
   "outputs": [],
   "source": [
    "scene = open(\"grail.txt\", \"r\")\n",
    "scene_one = scene.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:21:48.656630Z",
     "start_time": "2021-10-21T07:21:04.565641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anoop\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\anoop\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anoop\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in c:\\users\\anoop\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in c:\\users\\anoop\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:22:59.682945Z",
     "start_time": "2021-10-21T07:21:48.661658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anoop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:01.762941Z",
     "start_time": "2021-10-21T07:22:59.698082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'favor', 'walking', 'never', 'last', 'twenty', 'auntie', 'because', 'guiding', 'Just', 'finest', 'hidden', 'One', 'beat', 'acting', 'brain', 'been', 'round', 'mate', 'Please', 'Did', 'legs', 'Ulk', 'disheartened', 'raised', 'bugger-folk', 'soon', 'back', 'many', 'Haw', 'having', 'go', \"'is\", 'Mind', 'feint', 'left', 'you', 'thanks', 'Augh', 'bitching', 'sweet', 'another', 'Beast', 'language', 'least', 'creeper', 'SHRUBBER', 'Prince', 'ptoo', 'do', 'Even', 'ever', 'yet', 'wings', 'Oh', 'no', 'private', 'boil', 'Father', 'suffered', 'into', 'her', 'only', 'STUNNER', 'seem', 'looking', 'curtains', 'throughout', 'Unfortunately', 'ours', 'refuse', 'tracts', 'return', 'spam', 'welcome', 'what', 'killer', 'fair', 'clap', 'thwonk', 'That', 'Today', 'LUCKY', 'king-a', 'there', 'shrubbery', 'nose', 'Would', 'foul', 'profane', 'cope', 'girl', 'halves', '9', 'Camaaaaaargue', 'scratch', 'given', 'Tower', 'relax', 'kills', \"'S\", 'act', 'Excalibur', 'case', 'boys', 'bed-wetting', 'knows', 'remain', 'Remove', 'creep', 'MAYNARD', 'body', 'couple', 'turned', 'I', 'Churches', 'taunting', 'Five', 'Chaste', 'Britain', 'ran', 'Doctor', 'ratios', 'attend', 'lying', 'returns', 'Alright', 'this', 'MINSTREL', 'interested', 'harmless', 'grips', 'Exactly', 'laden', 'broken', 'alive', 'supports', 'fell', 'siren', 'woods', 'gouged', 'Ni', 'Anybody', 'dancing', 'help', \"'m\", 'mayhem', 'Oooohoohohooo', 'utterly', 'animal', 'tie', 'formidable', 'oh', 'obviously', 'executive', 'Hya', 'Yup', 'bother', 'awaaaaay', 'vicious', 'pweeng', 'groveling', 'continue', 'Practice', 'south', 'man', 'Providence', 'burn', 'ho', 'GIRLS', 'sign', 'kings', 'nightfall', 'strategy', 'under', 'France', 'Your', 'dying', 'brunettes', 'pulp', 'sons', 'Uuh', 'tops', 'Surely', 'pissing', 'his', 'necessary', 'joyful', 'knocked', 'WOMAN', 'hamster', 'Will', 'hang', 'hmm', 'set', 'Peril', 'Winter', 'order', 'signifying', 'uh', 'CRONE', 'Ow', 'orangutans', '4', \"d'you\", 'eh', 'Sorry', 'autonomous', 'ZOOT', 'hat', 'accent', 'prevent', 'thud', 'Thou', 'teeth', 'week', 'a', 'passed', 'purest', 'Every', 'air-speed', 'binding', 'vache', 'pass', 'new', 'automatically', 'Robinson', 'Galahad', '..', 'stops', 'Yeaaah', 'Maynard', 'Pin', 'sigh', 'sent', 'PRINCESS', 'doing', 'bastard', 'Pendragon', 'pussy', 'could', 'scenes', 'Hey', 'Charge', 'period', 'sheep', 'explain', 'lobbest', 'GUARD', 'Oooh', 'they', 'squeak', 'Joseph', 'carp', 'Black', 'thump', 'hello', 'in', 'Shh', 'open', 'counting', 'Yeaah', 'Open', 'VILLAGER', 'table', 'from', 'Clark', 'Get', 'rather', 'bunny', 'tackle', 'self-perpetuating', 'sniff', 'tinder', 'Mmm', 'VILLAGERS', 'OF', 'gone', 'plan', 'test', 'mangy', 'stretched', 'illegitimate-faced', 'Grail', '16', 'For', 'bows', 'rich', 'Ages', 'favorite', 'apart', 'well', 'routines', 'will', 'design', 'eccentric', ')', 'Lie', 'suggesting', 'jokes', 'he', '...', 'suit', '17', 'bats', 'HEAD', 'shall', 'Caerbannog', 'de', 'closest', 'undressing', 'GOD', 'Umhm', 'protect', 'higher', 'totally', 'second', 'happy', 'ways', 'buggered', 'resting', 'Hand', 'GUEST', 'Hang', 'Have', 'cost', 'dear', 'Huh', 'Yapping', 'aquatic', 'rhymes', 'fled', 'strange', 'Fiends', 'Shall', 'gravy', 'any', 'zone', 'fifty', 'Attila', 'alarm', 'advancing', 'Thpppppt', \"'\", 'hills', 'treat', 'breadth', 'ooh', 'vital', 'government', 'most', 'CRASH', 'long', 'Holy', 'Bread', 'velocity', 'bowels', 'face', 'cave', 'regulations', 'punishment', 'God', 'un', 'Which', 'using', 'guided', 'whop', 'Be', 'bum', 'cartoon', 'purpose', 'changed', 'stand', 'enjoying', 'safety', 'rope', 'questions', 'bits', 'trough', 'Cider', 'quests', 'through', 'head', 'own', 'shut', \"n't\", 'Follow', 'Swamp', 'knight', 'Aggh', 'um', \"'Til\", 'lady', 'Nine', 'Nador', 'The', 'like', 'donaeis', 'house', 'Shrubber', 'hall', 'watery', 'high-pitched', 'bonk', 'worse', 'enemies', 'Honestly', 'writing', 'see', 'Ooh', 'killed', 'chest', 'king', 'must', 'land', 'that', 'along', 'daughter', 'clllank', 'TIM', 'whoever', 'Hmm', 'Huy', 'awfully', 'sharp', 'master', 'oui', 'Gorge', 'Hallo', 'presence', 'performance', 'luck', 'yeah', 'burned', 'ROGER', 'horse', 'Knight', 'Away', 'Tell', 'good', \"'aaggggh\", 'asks', 'bite', 'warmer', 'Steady', 'hospital', 'door', 'DINGO', 'sure', 'looks', 'cadeau', \"'shrubberies\", 'Yes', 'kingdom', 'proceed', 'easily', 'kneeling', 'Fetchez', 'clang', 'Hah', 'turns', 'ROBIN', 'entered', 'forward', 'arrange', 'guarded', 'spirit', 'compared', 'Riiight', 'decision', 'strangers', 'Hiyah', 'guard', 'clank', \"'ll\", 'hoo', 'packing', 'twang', 'Pull', 'enchanter', 'Lancelot', 'even', 'carved', 'An', 'dark', 'servant', 'HERBERT', 'King', 'time', 'swallows', 'window-dresser', 'charged', 'nice-a', 'tap-dancing', 'coconuts', 'glad', 'stress', 'late', 'Nay', 'just', 'certainly', 'naughty', 'sense', 'chu', 'anyway', 'jump', 'Well', 'someone', 'hast', 'previous', 'samite', 'vote', 'union', 'coming', 'crone', 'proved', 'Uh', 'Wayy', 'Auuuuuuuugh', 'courage', 'spoken', 'Thppt', 'This', 'biscuits', 'parts', 'yes', 'giggle', 'ridden', 'stab', 'mooooooo', 'n', 'havin', 'very', 'tart', 'commune', 'sank', 'Silly', 'Throw', 'Or', 'day', 'side', 'wave', 'ninepence', 'ethereal', 'Aauuugh', 'Anarcho-syndicalism', 'blood', 'Bloody', 'somebody', 'French', 'fourth', 'Chickennn', 'ai', 'i', 'Iiiiives', 'gurgle', 'special', 'behaviour', 'excepting', 'HEADS', 'illustrious', 'Packing', 'carving', 'reasonable', 'wedding', 'Launcelot', 'freedom', 'for', 'unplugged', 'deeds', 'ride', 'things', 'Dis-mount', 'Monsieur', 'animator', 'mac', 'electric', 'minute', 'Ives', 'quite', 'model', 'room', 'How', 'Divine', 'pansy', 'him', 'person', 'somewhere', 'flesh', 'still', 'Perhaps', 'Ay', 'DEAD', 'ill.', 'trouble', 'diaphragm', 'cop', 'Thsss', 'use', 'die', 'sun', 'Iesu', 'music', 'worried', 'Aramaic', 'Is', 'pimples', 'trumpets', 'Not', 'going', 'became', 'immediately', 'Enchanter', 'spake', 'types', 'knock', 'Everything', 'wart', 'fold', 'art', 'supposed', \"'Oooooooh\", 'bold', 'we', 'Lake', 'Alice', 'eyes', 'thought', 'Our', 'did', 'Assyria', 'the-not-quite-so-brave-as-Sir-Lancelot', 'leads', 'progress', 'OFFICER', 'know', 'woman', 'Message', 'trade', 'has', 'sell', 'feel', 'My', 'Ha', 'tropical', 'dress', 'Said', 'wrong', 'aaugh', 'himself', 'Do', 'mightiest', 'give', 'dappy', 'makes', 'holy', 'seemed', 'evil', 'sometimes', 'Thpppt', 'Four', 'Who', 'outdoors', 'peril', 'Ho', 'Pie', 'throwing', 'have', 'longer', 'Allo', 'CARTOON', \"'forgive\", 'PARTY', 'out-clever', 'died', 'covered', 'clop', 'individually', 'bet', 'Now', 'friend', 'baaaa', 'By', 'His', 'witches', 'Midget', 'if', 'knights', 'Spring', 'life', 'supreme', 'able', 'violence', 'Antioch', 'actually', \"'Aauuuuugh\", 'PRISONER', 'Hyy', 'deal', 'pull', 'logically', 'pen', 'hopeless', \"'T\", 'temptress', 'kneecaps', ':', 'Order', 'k-nnnnniggets', 'lad', 'ehh', 'matter', '22', 'taken', 'mumble', 'brush', 'other', 'triumphs', 'indeed', 'derives', 'Wait', 'KNIGHTS', 'speak', 'Go', \"'Erbert\", 'dub', \"'Morning\", 'afoot', 'twin', 'which', 'little', 'minstrels', 'say', 'four', 'Stay', 'nibble', 'aaaaaah', 'Chicken', 'Bring', 'carry', 'was', 'north-east', 'distributing', '12', 'largest', '--', 'silly', 'stay', 'suddenly', 'handle', 'scarper', 'dynamite', 'Lead', 'ask', 'Eee', 'If', 'bottom', 'wan', 'count', 'beacon', 'agree', 'Aaaaaaaah', 'lovely', 'snore', 'worthy', 'one', 'Ah', 'command', 'move', 'Winston', 'earth', 'mystic', 'domine', 'warned', 'shrubber', 'Am', 'Consult', 'streak', 'fight', 'rrrr', 'found', 'haaa', 'cart', 'wise', 'Cornwall', 'humble', 'bottoms', 'Ahh', 'ere', 'smelt', 'Almighty', 'nearer', 'nearly', 'grenade', 'can', 'scales', 'sink', 'defeat', 'sacred', 'pond', 'reached', 'Heh', 'score', 'Zoot', 'handsome', 'threw', 'Forgive', 'employed', 'fire', 'bit', 'leap', 'oo', 'throat', 'awaaay', 'single-handed', 'nervous', 'mer', 'assist', ';', 'mandate', 'duck', '?', 'nick', 'slightly', 'better', 'how', 'would', 'breath', 'looney', 'duty', 'is', 'Must', 'lies', 'non-migratory', 'CUSTOMER', 'passing', 'unclog', 'force', 'More', 'stood', 'decided', '[', 'year', 'pointy', 'clad', 'Ridden', 'fought', 'Summer', 'creature', 'pig-dogs', 'PATSY', 'ounce', 'social', 'ha', 'GUARDS', 'entrance', 'worst', 'waste', 'starling', 'cross', 'Battle', 'blessing', 'after', 'towards', 'sworn', 'wind', 'Dappy', 'defeator', 'depressing', 'whom', 'Dramatically', 'Oooo', 'word', 'CROWD', 'fatal', 'Grenade', 'blow', 'sonny', 'preserving', 'reared', 'coconut', 'path', 'Frank', 'entering', 'MIDGET', 'honored', 'witness', 'Waa', 'sample', 'reads', 'chanting', 'bang', 'bringing', 'Shrubberies', \"'uuggggggh\", 'eight', 'Speak', 'outrageous', 'it', 'nobody', 'join', 'feet', 'basis', 'tea', 'next', 'repressing', \"'To\", 'shivering', 'uuggggggh', 'awaits', 'Quick', 'Knights', 'much', 'tit', 'woosh', 'everything', 'seek', 'Does', 'look', 'Uugh', 'blanket', 'Britons', 'simple', 'yourself', 'Peng', 'laurels', 'zoosh', 'emperor', 'badger', 'remembered', 'pounds', 'Aaaaaah', \"'Dennis\", 'quarrel', ',', 'further', 'anchovies', 'request', 'scrape', 'rodent', 'sloths', 'discovers', 'beside', 'bad-tempered', 'migrate', 'Off', 'and', 'Aaaah', 'always', 'already', 'Ohh', 'SCENE', 'such', 'swords', 'PIGLET', 'seems', 'small', 'castle', 'Mine', 'mercy', 'Are', 'howl', 'SIR', 'clue', 'inside', 'Chapter', 'forest', 'who', 'sponge', 'please', 'relics', 'me', 'radio', 'N', 'Since', 'bells', 'suffice', 'praised', 'Badon', 'be', 'meant', 'home', 'ladies', 'people', 'She', 'exciting', 'Castle', 'took', 'keepers', 'Y', 'bravely', 'Actually', 'then', 'bois', 'Lady', 'breakfast', 'anarcho-syndicalist', 'suspenseful', 'moment', 'nine', 'We', 'brave', 'bridge', 'valleys', 'underwear', 'argue', 'No', 'scribble', 'dunno', 'Aauuuves', 'dangerous', 'Silence', 'split', 'wicked', 'yel', 'doors', 'biters', 'stupid', 'miserable', 'impeccable', 'Other', 'conclusions', 'merger', 'science', 'boom', 'put', 'upon', 'dona', 'vain', 'real', 'So', 'lair', 'dare', 'leaps', 'ALL', 'persons', 'high', 'dorsal', 'NARRATOR', 'banana-shaped', 'pound', 'frighten', 'Cut', 'chastity"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 20439 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:01.798360Z",
     "start_time": "2021-10-21T07:23:01.762941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.search(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:01.999219Z",
     "start_time": "2021-10-21T07:23:01.802962Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:02.128535Z",
     "start_time": "2021-10-21T07:23:02.007816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:02.271674Z",
     "start_time": "2021-10-21T07:23:02.138527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:02.540060Z",
     "start_time": "2021-10-21T07:23:02.275669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@|#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:02.813064Z",
     "start_time": "2021-10-21T07:23:02.552818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer(tweets)\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:02.892809Z",
     "start_time": "2021-10-21T07:23:02.827658Z"
    }
   },
   "outputs": [],
   "source": [
    "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:03.152941Z",
     "start_time": "2021-10-21T07:23:02.892809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting world length with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:09.452831Z",
     "start_time": "2021-10-21T07:23:03.163062Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:11.332716Z",
     "start_time": "2021-10-21T07:23:09.463035Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = scene_one.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:12.003025Z",
     "start_time": "2021-10-21T07:23:11.332716Z"
    }
   },
   "outputs": [],
   "source": [
    "article_title = open(\"wiki_text_debugging.txt\", 'r')\n",
    "article = article_title.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:12.112914Z",
     "start_time": "2021-10-21T07:23:12.012828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:14.163099Z",
     "start_time": "2021-10-21T07:23:12.112914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anoop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Error with downloaded zip file\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anoop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T07:23:18.942987Z",
     "start_time": "2021-10-21T07:23:14.172934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stops = stopwords.words('english')\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
